# TnT-LLM Framework Implementation

This repository contains an implementation of the TnT-LLM (Taxonomy and Text Classification with Large Language Models) framework as described in the paper "TnT-LLM: Text Mining at Scale with Large Language Models".

This project is a Python-based solution for extracting, summarizing, and classifying climate-related documents into a taxonomy of tech-enabled solutions for climate adaptation and resilience. The code leverages OpenAI's GPT model, natural language processing (NLP) techniques, and machine learning classifiers like Logistic Regression, MLP, and LightGBM to generate and evaluate the taxonomy.

Hereâ€™s a detailed guide to understanding how the code works and how you can use it.

## Overview

This project consists of two primary phases:
1. Taxonomy Generation

This phase involves:

    Extracting text from climate-related documents (typically in PDF format).
    Summarizing the documents using GPT to provide concise summaries focused on identifying tech-enabled solutions for climate adaptation and resilience.
    Generating a taxonomy based on these summaries to categorize the different solutions into a well-structured classification.
    Evaluating the taxonomy on three key metrics:
        Coverage: How well the taxonomy captures the content from the documents.
        Label Accuracy: How accurate the labels generated by the taxonomy are.
        Relevance: How closely the taxonomy relates to the use case of climate adaptation and resilience.

2. LLM-Augmented Text Classification

Once the taxonomy is created, the project uses machine learning models to classify new documents. The steps involved are:

    Pseudo-labeling: Using the generated taxonomy to assign labels to the documents, which are then used as training labels.

    Training multiple classifiers:
        Logistic Regression
        MLP Classifier (Multilayer Perceptron)
        LightGBM

    These models are trained using the labeled documents and evaluated for performance.

    Evaluation: The models are evaluated using classification metrics like accuracy, F1-score, and classification reports. These results are saved for further analysis.

Key Functions
1. call_gpt_api(prompt: str, model: str = "gpt-4")

This function sends a prompt to OpenAI's GPT API and returns the generated response. It is used for summarizing documents and generating/refining the taxonomy.

def call_gpt_api(prompt: str, model: str = "gpt-4") -> str:
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000,
            top_p=1,
            n=1,
            stream=False,
            stop=None,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error calling GPT API: {e}")
        return ""

2. preprocess_text(text: str)

This function cleans and preprocesses the text by removing non-alphabetic characters and extra spaces. It ensures that the text is in a clean, usable format for training the models.

def preprocess_text(text: str) -> str:
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = " ".join(text.split())
    return text

3. extract_text_from_pdf(pdf_path: str)

This function extracts text from a given PDF file using PyMuPDF (fitz). It's designed to handle PDFs in the climate_plans folder.

def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        doc = fitz.open(pdf_path)
        text = "".join(page.get_text() for page in doc)
        return text
    except Exception as e:
        print(f"Error extracting text from PDF {pdf_path}: {e}")
        return ""

4. summarize_documents(documents: List[str], use_case: str)

This function uses GPT to generate a 20-word summary for each document based on the specified use case.

def summarize_documents(documents: List[str], use_case: str) -> List[str]:
    summaries = []
    for doc in documents:
        prompt = f"Summarize this document in about 20 words for the use case: {use_case}\n\n{doc[:2000]}"
        summary = call_gpt_api(prompt)
        summaries.append(summary)
    return summaries

```bash
python TnTprototype.py
